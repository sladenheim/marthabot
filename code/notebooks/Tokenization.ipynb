{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dd166c2-7823-497a-beda-04240de6b522",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "In this notebook we will demonstrate *tokenization*. This is the process of splitting raw text into smaller pieces, called (drum-roll please), *tokens*. Tokens can be individual characeters, words, or sentences.\n",
    "\n",
    "Examples of character and word tokenization are shown for the following raw text.\n",
    "\n",
    "```Show me the money```\n",
    "\n",
    "Character tokenization:\n",
    "\n",
    "```['S', 'h', 'o', 'w', 'm', 'e', 't', 'h', 'e', 'm', 'o', 'n', 'e', 'y']```.\n",
    "\n",
    "Word tokenization:\n",
    "\n",
    "```['Show', 'me', 'the', 'money'] ```\n",
    "\n",
    "We can achieve character and word tokenization very easily in Python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f01ca81-85be-4b3b-bf19-2cdc5f02cecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Show', 'me', 'the', 'money']\n",
      "['S', 'h', 'o', 'w', 'm', 'e', 't', 'h', 'e', 'm', 'o', 'n', 'e', 'y']\n"
     ]
    }
   ],
   "source": [
    "# Character and word tokenization\n",
    "\n",
    "sentence = \"Show me the money\"\n",
    "word_tokens = sentence.split()\n",
    "print(word_tokens)\n",
    "character_tokens = [char for char in sentence if char != ' ']\n",
    "print(character_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf1624c-a873-4a02-9f09-95c509acde52",
   "metadata": {},
   "source": [
    "There are advantages and disadvantages to different tokenization methods. We showed very simple strategies, but there are others that are more advanced. With tokenization, our goal is to not lose meaning with the tokens. With character based tokenization, especially for English (non-character based languages) we certainly lose meaning. \n",
    "\n",
    "We now demonstrate how to tokenize using the package [transformers](https://huggingface.co/docs/transformers/en/index) available from Huggingface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa0a2555-7676-400a-abe4-534519c99879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Show', 'me', 'the', 'money']\n",
      "['Let', \"'\", 's', 'try', 'to', 'see', 'if', 'we', 'can', 'get', 'this', 'transform', '##er', 'to', 'token', '##ize', '.']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\", \n",
    "                                          cache_dir=\"/projectnb/scottml/hf_cache\", \n",
    "                                          clean_up_tokenization_spaces=True)\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "print(tokens)\n",
    "\n",
    "# Try a more advanced sentence\n",
    "sentence2 = \"Let's try to see if we can get this transformer to tokenize.\"\n",
    "tokens2 = tokenizer.tokenize(sentence2)\n",
    "print(tokens2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f444fe7b-fdb7-45af-ac3f-622bbb467927",
   "metadata": {},
   "source": [
    "Associated to each token is a unique token ID. The total number of unique tokens that a model can recognize and process is the *vocabulary size*. The *vocabulary* is the collection of all the unique tokens (i.e., all the unique tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1fb1632-cd23-44a1-b111-662ccbbfb501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of tokens2 17\n",
      "Length of tokens2_ids 17\n",
      "[2421, 112, 188, 2222, 1106, 1267, 1191, 1195, 1169, 1243, 1142, 11303, 1200, 1106, 22559, 3708, 119]\n"
     ]
    }
   ],
   "source": [
    "tokens2_ids = tokenizer.convert_tokens_to_ids(tokens2)\n",
    "print(\"Length of tokens2\", len(tokens2))\n",
    "print(\"Length of tokens2_ids\", len(tokens2_ids))\n",
    "print(tokens2_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bed37f-65ea-4764-8cca-bed54985c9b8",
   "metadata": {},
   "source": [
    "The tokens (and token ids) alone hold no (semantic) information. There are different ways to *encode* this information numerically. A common  process is to create word embeddings. Word embeddings are the subject of a separate notebook. In words, token ids are transformed from an index value to a vectors in a high-dimensional space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cc8b52-b3f6-4ba8-a408-d6dc7ed4029e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
